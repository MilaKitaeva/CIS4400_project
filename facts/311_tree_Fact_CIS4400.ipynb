{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6fcb701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ETL Complaint Facts\n",
    "# If using the native Google BigQuery API module:\n",
    "from google.cloud import bigquery\n",
    "from google.cloud.exceptions import NotFound\n",
    "# import credentials\n",
    "import pandas as pd\n",
    "import os\n",
    "import pyarrow\n",
    "from datetime import datetime\n",
    "from google.oauth2 import service_account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60cd348d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame\n",
    "# Set the name of the dimension\n",
    "fact_name = 'requests'\n",
    "\n",
    "# Set the GCP Project, dataset and table name\n",
    "gcp_project = 'cis-4400-406318'\n",
    "bq_dataset = '311_newtree_dataset'\n",
    "table_name = fact_name + '_fact'\n",
    "# Construct the full BigQuery path to the table\n",
    "fact_table_path = \".\".join([gcp_project,bq_dataset,table_name])\n",
    "\n",
    "# Set the path to the source data files\n",
    "# For Linux use something like    /home/username/python_etl\n",
    "# For Mac use something like     /users/username/python_etl\n",
    "# file_source_path = 'c:\\\\Python_ETL'\n",
    "file_source_path = '/Users/liudmilakitaeva/311_tree_requests.csv'\n",
    "path_to_service_account_key_file = '/Users/liudmilakitaeva/Downloads/cis-4400-406318-6261595b1732.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cc596d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data( df):\n",
    "    \"\"\"\n",
    "    transform_data\n",
    "    Accepts a data frame\n",
    "    Performs any specific cleaning and transformation steps on the dataframe\n",
    "    Returns the modified dataframe\n",
    "    \"\"\"\n",
    "    # Convert the date to a datetime64 data type. 2012-08-21 04:12:16.827\n",
    "    df['created_date'] = pd.to_datetime(df['created_date'], format=\"%Y-%m-%d %H:%M:%S.%f\")\n",
    "    df['closed_date'] = pd.to_datetime(df['closed_date'], format=\"%Y-%m-%d %H:%M:%S.%f\")\n",
    "    # Convert the postal code into a string\n",
    "    df['zipcode'] =  df['zipcode'].astype(str)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8a8b90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_bigquery_table(bqclient, table_path, write_disposition, df):\n",
    "    \"\"\"\n",
    "    upload_bigquery_table\n",
    "    Accepts a path to a BigQuery table, the write disposition and a dataframe\n",
    "    Loads the data into the BigQuery table from the dataframe.\n",
    "    for credentials.\n",
    "    The write disposition is either\n",
    "    write_disposition=\"WRITE_TRUNCATE\"  Erase the target data and load all new data.\n",
    "    write_disposition=\"WRITE_APPEND\"    Append to the existing table\n",
    "    \"\"\"\n",
    "    try:\n",
    "        \n",
    "        job_config = bigquery.LoadJobConfig(write_disposition=write_disposition)\n",
    "        \n",
    "        # Submit the job\n",
    "        job = bqclient.load_table_from_dataframe(df, table_path, job_config=job_config)\n",
    "        \n",
    "        # Show the job results\n",
    "        job.result()\n",
    "    except Exception as err:\n",
    "        print(\"Failed to load BigQuery Table.\", err)\n",
    "        # os._exit(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cdbb4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigquery_table_exists(table_path, bqclient):\n",
    "    \"\"\"\n",
    "    bigquery_table_exists\n",
    "    Accepts a path to a BigQuery table\n",
    "    Checks if the BigQuery table exists.\n",
    "    Returns True or False\n",
    "    \"\"\"\n",
    "    try:\n",
    "        bqclient.get_table(table_path)  # Make an API request.\n",
    "        return True\n",
    "    except NotFound:\n",
    "        # print(\"Table {} is not found.\".format(table_id))\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bba81fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_new_table(bqclient, table_path, df):\n",
    "    \"\"\"\n",
    "    build_new_table\n",
    "    Accepts a path to a dimensional table, the dimension name and a data frame\n",
    "    Add the surrogate key and a record timestamp to the data frame\n",
    "    Inserts the contents of the dataframe to the dimensional table.\n",
    "    \"\"\"\n",
    "    upload_bigquery_table(bqclient, table_path, \"WRITE_TRUNCATE\", df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55222307",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_existing_table( bqclient, table_path, df):\n",
    "    \"\"\"\n",
    "    insert_existing_table\n",
    "    Accepts a path to a dimensional table, the dimension name and a data frame\n",
    "    Compares the new data to the existing data in the table.\n",
    "    Inserts the new/modified records to the existing table\n",
    "    \"\"\"\n",
    "    upload_bigquery_table( bqclient, table_path, \"WRITE_APPEND\", df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c979923",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_bigquery_table(table_path, bqclient, surrogate_key):\n",
    "    \"\"\"\n",
    "    query_bigquery_table\n",
    "    Accepts a path to a BigQuery table and the name of the surrogate key\n",
    "    Queries the BigQuery table but leaves out the update_timestamp and surrogate key columns\n",
    "    Returns the dataframe\n",
    "    \"\"\"    \n",
    "    bq_df = pd.DataFrame\n",
    "    # sql_query = 'SELECT * EXCEPT ( update_timestamp, '+surrogate_key+') FROM `' + table_path + '`'\n",
    "    sql_query = 'SELECT * FROM `' + table_path + '`'\n",
    "    bq_df = bqclient.query(sql_query).to_dataframe()\n",
    "    return bq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3a39ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dimension_lookup( dimension_name='agency', lookup_columns=['agency', 'agency_name'], df=df):\n",
    "    \"\"\"\n",
    "    dimension_lookup\n",
    "    Lookup the lookup_columns in the dimension_name and return the associated surrogate keys\n",
    "    Returns dataframe augmented with the surrogate keys\n",
    "    \"\"\"\n",
    "    bq_df = pd.DataFrame\n",
    "    surrogate_key = dimension_name+\"_dim_id\"\n",
    "    dimension_table_path = \".\".join([gcp_project,bq_dataset,dimension_name+\"_dimension\"])\n",
    "    # Fetch the existing table\n",
    "    bq_df = query_bigquery_table( dimension_table_path, bqclient, surrogate_key)\n",
    "    #print(bq_df)\n",
    "    # Melt the dimension dataframe into an index with the lookup columns\n",
    "    m = bq_df.melt(id_vars=lookup_columns, value_vars=surrogate_key)\n",
    "    #print(m)\n",
    "    # Rename the \"value\" column to the surrogate key column name\n",
    "    m=m.rename(columns={\"value\":surrogate_key})\n",
    "    # Merge with the fact table record\n",
    "    df = df.merge(m, on=lookup_columns, how='left')\n",
    "    # Drop the \"variable\" column and the lookup columns\n",
    "    df = df.drop(columns=lookup_columns)\n",
    "    df = df.drop(columns=\"variable\")\n",
    "    #print(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "575ce4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_dimension_lookup(dimension_name='date', lookup_column='created_date', df=df):\n",
    "    \"\"\"\n",
    "    date_dimension_lookup\n",
    "    Lookup the lookup_columns in a date dimension and return the associated surrogate keys\n",
    "    Returns dataframe augmented with the surrogate keys\n",
    "    \"\"\"\n",
    "    bq_df = pd.DataFrame\n",
    "    surrogate_key = dimension_name+\"_dim_id\"\n",
    "    dimension_table_path = \".\".join([gcp_project,bq_dataset,dimension_name+\"_dimension\"])\n",
    "    # Fetch the existing table\n",
    "    bq_df = query_bigquery_table( dimension_table_path, bqclient, surrogate_key)\n",
    "    bq_df['full_date'] = pd.to_datetime(bq_df['full_date'])\n",
    "    # Return just the date portion\n",
    "    bq_df[\"full_date\"] = bq_df.full_date.dt.date\n",
    "\n",
    "    # Dates in the 311 data look like this: 2017-08-11T11:57:00.000\n",
    "    # Extract the date from 'created_date' column\n",
    "    df[lookup_column] = pd.to_datetime(df[lookup_column])\n",
    "    # Return just the date portion\n",
    "    df[lookup_column] = df[lookup_column].dt.date\n",
    "\n",
    "    # Melt the dimension dataframe into an index with the lookup columns\n",
    "    m = bq_df.melt(id_vars='full_date', value_vars=surrogate_key)\n",
    "    # Rename the \"value\" column to the surrogate key column name\n",
    "    m=m.rename(columns={\"value\":lookup_column+\"_dim_id\"})\n",
    "\n",
    "    # Merge with the fact table record on the created_date\n",
    "    df = df.merge(m, left_on=lookup_column, right_on='full_date', how='left')\n",
    "\n",
    "    # Drop the \"variable\" column and the lookup columns\n",
    "    df = df.drop(columns=lookup_column)\n",
    "    df = df.drop(columns=\"variable\")\n",
    "    df = df.drop(columns=\"full_date\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f668a65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d8/g37q0y0x23d0v418qhh88hn40000gn/T/ipykernel_19814/2721573078.py:11: DtypeWarning: Columns (17,18,20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(data)\n",
      "/var/folders/d8/g37q0y0x23d0v418qhh88hn40000gn/T/ipykernel_19814/4171991975.py:18: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[lookup_column] = pd.to_datetime(df[lookup_column])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    df = pd.DataFrame\n",
    "    # Create the BigQuery Client\n",
    "    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = path_to_service_account_key_file\n",
    "\n",
    "    # Construct a BigQuery client object\n",
    "    bqclient = bigquery.Client()\n",
    "    \n",
    "    # Load in the data file\n",
    "    with open(file_source_path, 'r') as data:\n",
    "            df = pd.read_csv(data)\n",
    "        # Set all of the column names to lower case letters\n",
    "    #print(df.head())\n",
    "    df = df.rename(columns=str.lower)    \n",
    "    #df.location_type = df.location_type.fillna('other')\n",
    "    \n",
    "    # Consider removing columns that we will never use  df.drop([....])\n",
    "\n",
    "    # Lookup the agency dimension record  agency_dim_id\n",
    "    df = dimension_lookup( dimension_name='agency', lookup_columns=['agency', 'agency_name'], df=df)\n",
    "\n",
    "    # Lookup the location dimension record  location_dim_id\n",
    "    df = dimension_lookup( dimension_name='location', lookup_columns=['borough',  'latitude', 'longitude'], df=df)\n",
    "\n",
    "    # Lookup the channel  dimension record  channel_dim_id\n",
    "    df = dimension_lookup( dimension_name='status', lookup_columns=['status'], df=df)\n",
    "\n",
    "    # Lookup the complaint_type  dimension record  complaint_type_dim_id\n",
    "    #df = dimension_lookup dimension_name='complaint_type', lookup_columns=['complaint_type', 'descriptor'], df=df)\n",
    "\n",
    "    # Lookup the time dimension record using the time part of the created_date\n",
    "    # Note - do this before looking up the date dimension\n",
    "    #df = time_dimension_lookup( dimension_name='time', lookup_column='created_date', df=df)\n",
    "    # The time_dimension_lookup returns a column named 'time_dim_id'. Rename this to the 'created_time_dim_id'\n",
    "    #df = df.rename(columns={'time_dim_id' : 'created_time_dim_id'})\n",
    "\n",
    "    # Lookup the created_date dimension record\n",
    "    df = date_dimension_lookup(dimension_name='date', lookup_column='created_date', df=df)\n",
    "\n",
    "    # Lookup the closed_date dimension record\n",
    "    df = date_dimension_lookup(dimension_name='date', lookup_column='closed_date', df=df)\n",
    " # A list of all of the surrogate keys\n",
    "    # For transaction grain, also include the 'unique_key' column\n",
    "    surrogate_keys=['agency_dim_id','location_dim_id','created_date_dim_id','status_dim_id','closed_date_dim_id', 'unique_key']\n",
    "\n",
    "    # Remove all of the other non-surrogate key columns\n",
    "    df = df[surrogate_keys]\n",
    "\n",
    "    # For daily snapshot grain we:\n",
    "    # 1) Add a 'complaint_count' fact\n",
    "    # 2) Use Group By to count up the number of complaints, per location, per agency, etc. per day\n",
    "    # For transaction grain add in the unique_key but skip the above two steps.\n",
    "\n",
    "    # Add a complaint count (for daily snapshot grain)\n",
    "    #df['requests_count'] = 1\n",
    "    # Count up the number of complaints per agency, per location, etc. per day\n",
    "    #df = df.groupby(surrogate_keys)['requests_count'].agg('count').reset_index()\n",
    "\n",
    "    # See if the target table exists\n",
    "    target_table_exists = bigquery_table_exists(fact_table_path, bqclient )\n",
    "    # If the target table does not exist, load all of the data into a new table\n",
    "    if not target_table_exists:\n",
    "        build_new_table( bqclient, fact_table_path, df)\n",
    "    # If the target table exists, then perform an incremental load\n",
    "    if target_table_exists:\n",
    "        insert_existing_table( bqclient, fact_table_path, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04701726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agency_dim_id</th>\n",
       "      <th>location_dim_id</th>\n",
       "      <th>created_date_dim_id</th>\n",
       "      <th>status_dim_id</th>\n",
       "      <th>closed_date_dim_id</th>\n",
       "      <th>unique_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2886</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>59544918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2886</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>59543872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2886</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>59549161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2886</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>59549163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2886</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>59550214</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   agency_dim_id  location_dim_id  created_date_dim_id  status_dim_id   \n",
       "0              1                1                 2886              1  \\\n",
       "1              1                2                 2886              1   \n",
       "2              1                3                 2886              1   \n",
       "3              1                3                 2886              1   \n",
       "4              1                4                 2886              1   \n",
       "\n",
       "   closed_date_dim_id  unique_key  \n",
       "0                <NA>    59544918  \n",
       "1                <NA>    59543872  \n",
       "2                <NA>    59549161  \n",
       "3                <NA>    59549163  \n",
       "4                <NA>    59550214  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
